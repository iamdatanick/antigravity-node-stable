# Antigravity Node v14.1 — Ship It

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Connect the existing infrastructure into a working end-to-end system: fix the broken UI, add document→RAG pipeline, wire real MCP tools, and deploy.

**Architecture:**
- FastAPI orchestrator (`workflows/a2a_server.py`) with AsyncDAGEngine (`src/orchestrator/engine.py`) for task management
- etcd3 for distributed locking (document dedup), aioboto3/Ceph for artifact persistence
- Upload → parse (pypdf, python-docx) → chunk → embed (budget-proxy → OpenAI) → store (ChromaDB)
- Chat → query ChromaDB → inject RAG context → stream via budget-proxy → cloud LLMs or OVMS local
- MCP tools backed by real services: document_search, upload, inference, chat, health

**What Already Exists (use it, don't rebuild):**

| Component | File | What It Does |
|-----------|------|--------------|
| FastAPI orchestrator | `workflows/a2a_server.py` | 15+ endpoints: /task, /upload, /health, /v1/chat/completions, etc. |
| AsyncDAGEngine | `src/orchestrator/engine.py` | etcd3 locking, aioboto3 artifacts, task submit/status/list |
| OVMS inference | `workflows/inference.py` | gRPC (9000) + REST (9001) to OpenVINO Model Server |
| Budget proxy | `src/budget-proxy/proxy.py` | LLM routing (OpenAI/Anthropic/local), cost tracking, /v1/models |
| S3 client | `workflows/s3_client.py` | Ceph RGW wrapper (upload/download/list) |
| Stream chat | `src/master-ui/src/api/client.ts` | streamChat(), uploadFile(), apiFetch() |
| Chat store | `src/master-ui/src/stores/chatStore.ts` | Zustand store (messages, streaming state) |

**Approved Packages:**

| Package | Version | License | Foundation | Status |
|---------|---------|---------|------------|--------|
| etcd3 | 0.12.0 | Apache-2.0 | Independent | APPROVED (in use) |
| aioboto3 | 11.3.0 | Apache-2.0 | Independent | APPROVED (in use) |
| a2a-python | 0.3.0 | Apache-2.0 | Linux Foundation | APPROVED |
| openvino-genai | 2025.4.0 | Apache-2.0 | Intel OSS | APPROVED |
| optimum-intel | 1.22.0 | Apache-2.0 | Intel OSS | APPROVED |
| fastapi | 0.115.6 | MIT | Independent | APPROVED (in use) |
| grpcio | 1.70.0 | Apache-2.0 | CNCF | APPROVED (in use) |
| pydantic | 2.10.5 | MIT | Independent | APPROVED (in use) |
| tenacity | 8.2.3 | Apache-2.0 | Independent | APPROVED (in use) |
| boto3 | 1.35.0 | Apache-2.0 | Independent | APPROVED (in use) |
| chromadb-client | 0.5.23 | Apache-2.0 | Independent | APPROVED (new) |
| pypdf | 4.0 | BSD-3-Clause | Independent | APPROVED (new) |
| python-docx | 1.1 | MIT | Independent | APPROVED (new) |
| asyncpg | REMOVED | N/A | N/A | REMOVED |

**K8s/Argo → v14.1 Replacements (already built):**

| K8s/Argo Feature | v14.1 Replacement | Where |
|------------------|-------------------|-------|
| Persistence claims | Docker volumes + Ceph S3 | `docker-compose.yml` volumes |
| Secret injection | OpenBao vault (port 8200) | `budget-proxy/_fetch_vault_key()` |
| Service discovery | Docker Compose DNS | Container names as hostnames |
| Workflow engine | AsyncDAGEngine | `src/orchestrator/engine.py` |
| Distributed locks | etcd3 leases | `engine.acquire_lock()` |
| Artifact storage | aioboto3 → Ceph RGW | `engine.store_artifact()` |

---

## Phase 1: Fix the Broken UI

### Task 1: Fix Chat/AppShell z-index collision

**Files:**
- Modify: `src/master-ui/src/pages/Chat.tsx:116,156,165,173,182`
- Modify: `src/master-ui/src/components/layout/AppShell.tsx:11`
- Modify: `src/master-ui/src/pages/Dashboard.tsx` (add `p-6` wrapper)
- Modify: `src/master-ui/src/pages/Budget.tsx` (add `p-6` wrapper)
- Modify: `src/master-ui/src/pages/Services.tsx` (add `p-6` wrapper)
- Modify: `src/master-ui/src/pages/Settings.tsx` (add `p-6` wrapper)

**Problem:** Chat.tsx uses `className="fixed inset-0"` but it's NESTED inside AppShell's `<Sidebar> + <TopBar> + <main className="flex-1 overflow-y-auto p-6">`. The `fixed` breaks out of the flex layout and covers sidebar/topbar. All child components (NeuralInput, AmbientHUD, CLEAR button) also use `fixed` positioning.

**Step 1: Fix Chat.tsx — change `fixed` to `absolute` throughout**

Line 116:
```tsx
// OLD:
<div className="fixed inset-0 overflow-hidden bg-[var(--color-bg-primary)]"

// NEW:
<div className="absolute inset-0 overflow-hidden bg-[var(--color-bg-primary)]"
```

Line 182 (CLEAR button):
```tsx
// OLD:
className="fixed top-4 left-1/2 ..."

// NEW:
className="absolute top-4 left-1/2 ..."
```

Also update NeuralInput.tsx and AmbientHUD.tsx: any `fixed` → `absolute`.

**Step 2: Make AppShell's `<main>` a positioning context, remove padding**

`AppShell.tsx` line 11:
```tsx
// OLD:
<main className="flex-1 overflow-y-auto p-6">{children}</main>

// NEW:
<main className="flex-1 overflow-y-auto relative">{children}</main>
```

**Step 3: Add `p-6` wrapper to non-Chat pages**

Each of Dashboard.tsx, Budget.tsx, Services.tsx, Settings.tsx: wrap the outermost content in `<div className="p-6">`.

Unavailable.tsx uses full-height centering — no padding needed.

**Step 4: Build and verify**

Run: `cd "OneDrive/Desktop/Antigravity-Node/src/master-ui" && npm run build`
Expected: 0 errors

**Step 5: Commit**

```
git add src/master-ui/src/pages/Chat.tsx src/master-ui/src/components/layout/AppShell.tsx \
  src/master-ui/src/pages/Dashboard.tsx src/master-ui/src/pages/Budget.tsx \
  src/master-ui/src/pages/Services.tsx src/master-ui/src/pages/Settings.tsx \
  src/master-ui/src/components/chat/NeuralInput.tsx src/master-ui/src/components/chat/AmbientHUD.tsx
git commit -m "fix: chat z-index collision — fixed→absolute, remove AppShell p-6"
```

---

### Task 2: Fix version strings, remove dead code

**Files:**
- Modify: `src/master-ui/src/components/layout/TopBar.tsx:53`
- Modify: `src/master-ui/package.json:4,13-20,31`

**Step 1: Fix TopBar version**

Line 53: `v13.0` → `v14.1`

**Step 2: Fix package.json version**

Line 4: `"13.0.0"` → `"14.1.0"`

**Step 3: Remove dead dependencies from package.json**

Remove from `dependencies`:
```
"@codemirror/lang-sql": "^6.10.0",
"@codemirror/state": "^6.5.4",
"@codemirror/theme-one-dark": "^6.1.3",
"@codemirror/view": "^6.39.13",
"@xterm/addon-fit": "^0.11.0",
"@xterm/xterm": "^6.0.0",
"cytoscape": "^3.33.1",
```

Remove from `devDependencies`:
```
"@types/cytoscape": "^3.21.9",
```

**Step 4: Reinstall and build**

Run: `cd "OneDrive/Desktop/Antigravity-Node/src/master-ui" && npm install && npm run build`

**Step 5: Commit**

```
git add src/master-ui/src/components/layout/TopBar.tsx src/master-ui/package.json src/master-ui/package-lock.json
git commit -m "fix: TopBar v13.0→v14.1, remove dead deps (codemirror/xterm/cytoscape)"
```

---

### Task 3: Add chat persistence (localStorage)

**Files:**
- Modify: `src/master-ui/src/stores/chatStore.ts`

**Step 1: Add Zustand `persist` middleware**

```ts
import { create } from "zustand";
import { persist } from "zustand/middleware";

export interface Attachment {
  name: string;
  size: number;
  key: string;
  type: string;
  content?: string;
}

export interface ChatMessage {
  id: string;
  role: "user" | "assistant" | "system";
  content: string;
  timestamp: number;
  thinking?: string[];
  tools?: { name: string; result: string }[];
  streaming?: boolean;
  attachments?: Attachment[];
}

interface ChatState {
  messages: ChatMessage[];
  isStreaming: boolean;
  addMessage: (msg: Omit<ChatMessage, "id" | "timestamp">) => string;
  updateMessage: (id: string, partial: Partial<ChatMessage>) => void;
  appendContent: (id: string, chunk: string) => void;
  setStreaming: (v: boolean) => void;
  clear: () => void;
}

let msgId = Date.now();

export const useChatStore = create<ChatState>()(
  persist(
    (set) => ({
      messages: [],
      isStreaming: false,
      addMessage: (msg) => {
        const id = `msg-${++msgId}`;
        set((s) => ({
          messages: [...s.messages, { ...msg, id, timestamp: Date.now() }],
        }));
        return id;
      },
      updateMessage: (id, partial) =>
        set((s) => ({
          messages: s.messages.map((m) => (m.id === id ? { ...m, ...partial } : m)),
        })),
      appendContent: (id, chunk) =>
        set((s) => ({
          messages: s.messages.map((m) =>
            m.id === id ? { ...m, content: m.content + chunk } : m,
          ),
        })),
      setStreaming: (isStreaming) => set({ isStreaming }),
      clear: () => {
        msgId = Date.now();
        set({ messages: [], isStreaming: false });
      },
    }),
    {
      name: "antigravity-chat",
      partialize: (state) => ({
        messages: state.messages.filter((m) => !m.streaming),
      }),
    },
  ),
);
```

**Step 2: Build**

Run: `cd "OneDrive/Desktop/Antigravity-Node/src/master-ui" && npm run build`

**Step 3: Commit**

```
git add src/master-ui/src/stores/chatStore.ts
git commit -m "feat: chat persistence via localStorage (Zustand persist)"
```

---

## Phase 2: Document Processing Pipeline

### Task 4: Add document parsing and vector deps

**Files:**
- Modify: `deployment/cloud-test/requirements.txt`

**Step 1: Append new dependencies**

Add to end of `deployment/cloud-test/requirements.txt`:
```
# Document Processing (RAG Pipeline)
pypdf>=4.0
python-docx>=1.1

# Vector Store Client (Apache-2.0)
chromadb-client>=0.5.0
```

**Step 2: Commit**

```
git add deployment/cloud-test/requirements.txt
git commit -m "deps: add pypdf, python-docx, chromadb-client for RAG pipeline"
```

---

### Task 5: Create document processor

**Files:**
- Create: `workflows/document_processor.py`
- Create: `tests/test_document_processor.py`

**Step 1: Write the failing test**

```python
"""Tests for document parsing and chunking."""

from workflows.document_processor import chunk_text, extract_text


class TestExtractText:
    def test_plain_text(self):
        result = extract_text("test.txt", b"Hello world. This is a test.")
        assert "Hello world" in result

    def test_empty_content(self):
        assert extract_text("test.txt", b"") == ""

    def test_unsupported_format(self):
        assert extract_text("test.exe", b"\x00\x01\x02") == ""

    def test_markdown(self):
        result = extract_text("readme.md", b"# Title\n\nParagraph text.")
        assert "Title" in result


class TestChunkText:
    def test_empty_text(self):
        assert chunk_text("") == []
        assert chunk_text("   ") == []

    def test_short_text_single_chunk(self):
        chunks = chunk_text("Short text here.", chunk_size=1000)
        assert len(chunks) <= 1

    def test_long_text_multiple_chunks(self):
        text = "Word " * 500
        chunks = chunk_text(text, chunk_size=500, overlap=100)
        assert len(chunks) > 1
        for chunk in chunks:
            assert len(chunk) >= 50

    def test_no_tiny_fragments(self):
        text = "A" * 30 + " " + "B" * 200
        chunks = chunk_text(text, chunk_size=100, overlap=20)
        for chunk in chunks:
            assert len(chunk) >= 50
```

**Step 2: Run test to verify it fails**

Run: `cd "OneDrive/Desktop/Antigravity-Node" && python -m pytest tests/test_document_processor.py -v`
Expected: ImportError

**Step 3: Write the implementation**

```python
"""Document parsing and chunking for RAG pipeline.

Extracts text from PDF, DOCX, and plain-text files, then splits into
overlapping chunks suitable for embedding and vector storage.
"""

import io
import logging
from pathlib import Path

logger = logging.getLogger("antigravity.document")

_TEXT_EXTS = frozenset({
    ".txt", ".md", ".csv", ".json", ".yaml", ".yml",
    ".py", ".js", ".ts", ".tsx", ".html", ".xml", ".log",
})


def extract_text(filename: str, content: bytes) -> str:
    """Extract text from a file based on its extension."""
    if not content:
        return ""

    ext = Path(filename).suffix.lower()

    if ext == ".pdf":
        return _extract_pdf(content)
    if ext in (".docx", ".doc"):
        return _extract_docx(content)
    if ext in _TEXT_EXTS:
        return content.decode("utf-8", errors="replace")

    logger.warning("Unsupported file type for text extraction: %s", ext)
    return ""


def _extract_pdf(content: bytes) -> str:
    from pypdf import PdfReader

    reader = PdfReader(io.BytesIO(content))
    return "\n\n".join(
        page.extract_text() or "" for page in reader.pages
    )


def _extract_docx(content: bytes) -> str:
    from docx import Document

    doc = Document(io.BytesIO(content))
    return "\n\n".join(p.text for p in doc.paragraphs if p.text.strip())


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:
    """Split text into overlapping chunks for embedding.

    Tries to break at paragraph/sentence boundaries.
    Skips chunks shorter than 50 characters.
    """
    text = text.strip()
    if not text:
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size

        if end < len(text):
            for sep in ["\n\n", "\n", ". ", " "]:
                pos = text.rfind(sep, start + chunk_size // 2, end + 100)
                if pos > start:
                    end = pos + len(sep)
                    break

        chunk = text[start:end].strip()
        if len(chunk) >= 50:
            chunks.append(chunk)
        start = end - overlap

    return chunks
```

**Step 4: Run test to verify it passes**

Run: `cd "OneDrive/Desktop/Antigravity-Node" && python -m pytest tests/test_document_processor.py -v`
Expected: 7 passed

**Step 5: Commit**

```
git add workflows/document_processor.py tests/test_document_processor.py
git commit -m "feat: document processor — parse PDF/DOCX/text + chunking"
```

---

### Task 6: Add /v1/embeddings to budget-proxy

**Files:**
- Modify: `src/budget-proxy/proxy.py`
- Modify: `tests/test_budget_proxy.py`

**Step 1: Write the failing test**

Add to `tests/test_budget_proxy.py`:
```python
class TestEmbeddingsEndpoint:
    @pytest.mark.asyncio
    async def test_embeddings_no_key_returns_503(self):
        """Embeddings endpoint returns 503 when no API key is available."""
        async with AsyncClient(transport=ASGITransport(app=app), base_url="http://test") as client:
            response = await client.post(
                "/v1/embeddings",
                json={"input": ["test text"], "model": "text-embedding-3-small"},
            )
            assert response.status_code == 503
```

**Step 2: Run to verify it fails (404 — endpoint doesn't exist yet)**

**Step 3: Add embedding endpoint to `src/budget-proxy/proxy.py`**

After the `/v1/models` endpoint:

```python
@app.post("/v1/embeddings")
async def create_embeddings(request: Request):
    """Route embedding requests to OpenAI — budget-tracked."""
    global _daily_spend

    async with _spend_lock:
        _reset_if_new_day()
        if _daily_spend >= DAILY_BUDGET_USD:
            raise HTTPException(status_code=429, detail="Daily budget exhausted")

    body = await request.json()
    model = body.get("model", "text-embedding-3-small")

    key = OPENAI_API_KEY or await _fetch_vault_key("openai")
    if not key:
        raise HTTPException(status_code=503, detail="No embedding API key available")

    async with httpx.AsyncClient(timeout=30.0) as client:
        resp = await client.post(
            "https://api.openai.com/v1/embeddings",
            json=body,
            headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        )

    if resp.status_code != 200:
        return JSONResponse(status_code=resp.status_code, content=resp.json())

    result = resp.json()
    usage = result.get("usage", {})
    cost = usage.get("total_tokens", 0) / 1_000_000 * 0.02

    async with _spend_lock:
        _daily_spend += cost
        _update_hourly(cost)

    return JSONResponse(content=result)
```

Also add to COST_TABLE:
```python
"text-embedding-3-small": {"input": 0.00002, "output": 0.0},
```

**Step 4: Run tests**

Run: `cd "OneDrive/Desktop/Antigravity-Node" && python -m pytest tests/test_budget_proxy.py -v`

**Step 5: Commit**

```
git add src/budget-proxy/proxy.py tests/test_budget_proxy.py
git commit -m "feat: budget-proxy /v1/embeddings endpoint with cost tracking"
```

---

### Task 7: Add ChromaDB container

**Files:**
- Modify: `deployment/cloud-test/docker-compose.yml`

**Step 1: Add ChromaDB service**

After `budget-proxy`, before `ui`:

```yaml
  # Vector Store (Apache-2.0) — RAG document search
  chromadb:
    image: chromadb/chroma:0.5.23
    container_name: chromadb
    volumes:
      - ./data/chroma:/chroma/chroma
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5
```

**Step 2: Add CHROMA env vars to orchestrator and add to depends_on**

```yaml
  orchestrator:
    environment:
      # ... existing env vars ...
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
    depends_on:
      # ... existing ...
      chromadb:
        condition: service_healthy
```

**Step 3: Commit**

```
git add deployment/cloud-test/docker-compose.yml
git commit -m "infra: add ChromaDB container for RAG vector storage"
```

---

### Task 8: Create vector store client

**Files:**
- Create: `workflows/vector_store.py`
- Create: `tests/test_vector_store.py`

**Step 1: Write the failing test**

```python
"""Tests for vector store client (mocked ChromaDB)."""

from unittest.mock import MagicMock, patch


@patch("workflows.vector_store.chromadb")
class TestVectorStore:
    def test_store_chunks(self, mock_chroma):
        from workflows.vector_store import store_chunks

        mock_collection = MagicMock()
        mock_client = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_chroma.HttpClient.return_value = mock_client

        # Reset singleton
        import workflows.vector_store as vs
        vs._client = None

        store_chunks("tenant-1", "doc/test.pdf", ["chunk1", "chunk2"], [[0.1] * 10, [0.2] * 10])

        mock_collection.upsert.assert_called_once()
        args = mock_collection.upsert.call_args
        assert len(args.kwargs["ids"]) == 2
        assert len(args.kwargs["documents"]) == 2

    def test_search_returns_hits(self, mock_chroma):
        from workflows.vector_store import search

        mock_collection = MagicMock()
        mock_collection.query.return_value = {
            "documents": [["relevant chunk"]],
            "metadatas": [[{"source": "doc.pdf", "chunk_index": 0}]],
            "distances": [[0.15]],
        }
        mock_client = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_chroma.HttpClient.return_value = mock_client

        import workflows.vector_store as vs
        vs._client = None

        hits = search("tenant-1", [0.1] * 10, n_results=3)

        assert len(hits) == 1
        assert hits[0]["content"] == "relevant chunk"
        assert hits[0]["source"] == "doc.pdf"
        assert hits[0]["distance"] == 0.15
```

**Step 2: Run test, verify fail**

**Step 3: Write implementation**

```python
"""ChromaDB vector store client for RAG pipeline.

Connects to ChromaDB container via HTTP. Stores document chunks with
pre-computed embeddings and provides cosine similarity search.
"""

import logging
import os

import chromadb

logger = logging.getLogger("antigravity.vectors")

CHROMA_HOST = os.environ.get("CHROMA_HOST", "chromadb")
CHROMA_PORT = int(os.environ.get("CHROMA_PORT", "8000"))

_client = None


def _get_client():
    global _client
    if _client is None:
        _client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    return _client


def get_collection(tenant_id: str = "default"):
    """Get or create a tenant-scoped vector collection."""
    client = _get_client()
    return client.get_or_create_collection(
        name=f"docs_{tenant_id}",
        metadata={"hnsw:space": "cosine"},
    )


def store_chunks(
    tenant_id: str,
    doc_key: str,
    chunks: list[str],
    embeddings: list[list[float]],
):
    """Store document chunks with pre-computed embeddings."""
    collection = get_collection(tenant_id)
    ids = [f"{doc_key}::chunk-{i}" for i in range(len(chunks))]
    collection.upsert(
        ids=ids,
        documents=chunks,
        embeddings=embeddings,
        metadatas=[{"source": doc_key, "chunk_index": i} for i in range(len(chunks))],
    )
    logger.info("Stored %d chunks for %s in tenant=%s", len(chunks), doc_key, tenant_id)


def search(
    tenant_id: str,
    query_embedding: list[float],
    n_results: int = 5,
) -> list[dict]:
    """Search for relevant chunks by embedding similarity."""
    collection = get_collection(tenant_id)
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results,
    )
    hits = []
    for i, doc in enumerate(results["documents"][0]):
        hits.append({
            "content": doc,
            "source": results["metadatas"][0][i].get("source", ""),
            "distance": results["distances"][0][i] if results.get("distances") else None,
        })
    return hits
```

**Step 4: Run tests, verify pass**

**Step 5: Commit**

```
git add workflows/vector_store.py tests/test_vector_store.py
git commit -m "feat: ChromaDB vector store client (store + search)"
```

---

### Task 9: Create embedding client

**Files:**
- Create: `workflows/embedding_client.py`
- Create: `tests/test_embedding_client.py`

**Step 1: Write the failing test**

```python
"""Tests for embedding client."""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest


@pytest.mark.asyncio
@patch("workflows.embedding_client.httpx.AsyncClient")
async def test_embed_texts(mock_client_cls):
    from workflows.embedding_client import embed_texts

    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.raise_for_status = MagicMock()
    mock_response.json.return_value = {
        "data": [
            {"embedding": [0.1, 0.2], "index": 0},
            {"embedding": [0.3, 0.4], "index": 1},
        ],
    }

    mock_client = AsyncMock()
    mock_client.__aenter__ = AsyncMock(return_value=mock_client)
    mock_client.__aexit__ = AsyncMock(return_value=False)
    mock_client.post = AsyncMock(return_value=mock_response)
    mock_client_cls.return_value = mock_client

    result = await embed_texts(["hello", "world"])

    assert len(result) == 2
    assert result[0] == [0.1, 0.2]
    assert result[1] == [0.3, 0.4]


@pytest.mark.asyncio
@patch("workflows.embedding_client.httpx.AsyncClient")
async def test_embed_texts_preserves_order(mock_client_cls):
    """Embeddings returned out-of-order get sorted by index."""
    from workflows.embedding_client import embed_texts

    mock_response = MagicMock()
    mock_response.raise_for_status = MagicMock()
    mock_response.json.return_value = {
        "data": [
            {"embedding": [0.9], "index": 1},
            {"embedding": [0.1], "index": 0},
        ],
    }

    mock_client = AsyncMock()
    mock_client.__aenter__ = AsyncMock(return_value=mock_client)
    mock_client.__aexit__ = AsyncMock(return_value=False)
    mock_client.post = AsyncMock(return_value=mock_response)
    mock_client_cls.return_value = mock_client

    result = await embed_texts(["a", "b"])
    assert result[0] == [0.1]
    assert result[1] == [0.9]
```

**Step 2: Run test, verify fail**

**Step 3: Write implementation**

```python
"""Client for generating embeddings via budget-proxy.

Routes through budget-proxy to maintain cost tracking and budget controls.
Budget-proxy handles API key management via OpenBao vault.
"""

import logging
import os

import httpx

logger = logging.getLogger("antigravity.embeddings")

BUDGET_PROXY_URL = os.environ.get("LITELLM_BASE_URL", "http://budget-proxy:4055")


async def embed_texts(
    texts: list[str],
    model: str = "text-embedding-3-small",
) -> list[list[float]]:
    """Generate embeddings for a list of texts via budget-proxy."""
    async with httpx.AsyncClient(timeout=60.0) as client:
        resp = await client.post(
            f"{BUDGET_PROXY_URL}/v1/embeddings",
            json={"input": texts, "model": model},
        )
        resp.raise_for_status()
        data = resp.json()
        sorted_items = sorted(data["data"], key=lambda x: x["index"])
        return [item["embedding"] for item in sorted_items]
```

**Step 4: Run tests, verify pass**

**Step 5: Commit**

```
git add workflows/embedding_client.py tests/test_embedding_client.py
git commit -m "feat: embedding client — routes through budget-proxy"
```

---

### Task 10: Wire upload → parse → chunk → embed → store

**Files:**
- Modify: `workflows/a2a_server.py` (upload_file endpoint, lines 286-320)

**Step 1: Write the test**

Add to `tests/test_endpoints_pydantic.py`:
```python
class TestUploadRAGPipeline:
    @patch("workflows.a2a_server._process_document", new_callable=AsyncMock)
    @patch("workflows.a2a_server.s3_upload")
    @patch("workflows.a2a_server.push_episodic")
    def test_upload_triggers_processing(self, mock_push, mock_s3, mock_process, client):
        """Upload triggers background document processing."""
        mock_s3.return_value = None
        response = client.post(
            "/upload",
            files={"file": ("test.pdf", b"fake pdf", "application/pdf")},
            headers={"x-tenant-id": "tenant-1"},
        )
        assert response.status_code == 200
        mock_process.assert_called_once()
```

**Step 2: Run test, verify fail**

**Step 3: Add `_process_document` function and wire into upload**

Add to `workflows/a2a_server.py`:

```python
async def _process_document(key: str, filename: str, content: bytes, tenant_id: str):
    """Background: parse → chunk → embed → store in ChromaDB.

    Uses AsyncDAGEngine distributed lock to prevent duplicate processing.
    Stores processing result as artifact via aioboto3/Ceph.
    """
    try:
        from workflows.document_processor import extract_text, chunk_text
        from workflows.embedding_client import embed_texts
        from workflows.vector_store import store_chunks

        text = extract_text(filename, content)
        if not text:
            logger.info("No text extracted from %s — skipping RAG indexing", filename)
            return

        chunks = chunk_text(text)
        if not chunks:
            logger.info("No chunks from %s", filename)
            return

        # Distributed lock via etcd3 (prevent duplicate processing)
        lock = await engine.acquire_lock(f"doc-process-{key}", ttl=120)
        try:
            embeddings = await embed_texts(chunks)
            store_chunks(tenant_id, key, chunks, embeddings)
            logger.info("RAG indexed %s: %d chunks", filename, len(chunks))

            # Store processing artifact via aioboto3/Ceph
            import json as _json
            await engine.store_artifact(
                task_id=key.replace("/", "_"),
                key="rag_result.json",
                data=_json.dumps({"chunks": len(chunks), "filename": filename}).encode(),
                tenant_id=tenant_id,
            )
        finally:
            await engine.release_lock(f"doc-process-{key}", lock)

    except Exception as e:
        logger.warning("Document processing failed for %s: %s", filename, e)
```

In the `upload_file` endpoint, after `s3_upload(key, content)`, add:

```python
    # Background: parse → chunk → embed → store in vector DB
    asyncio.create_task(_process_document(key, safe_filename, content, x_tenant_id))
```

**Step 4: Run tests, verify pass**

**Step 5: Commit**

```
git add workflows/a2a_server.py tests/test_endpoints_pydantic.py
git commit -m "feat: upload triggers RAG pipeline (parse→chunk→embed→store)"
```

---

## Phase 3: RAG-Augmented Chat

### Task 11: Add /search endpoint

**Files:**
- Modify: `workflows/a2a_server.py`
- Modify: `workflows/models.py` (add SearchRequest/SearchResponse)

**Step 1: Add Pydantic models to `workflows/models.py`**

```python
class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1)
    n: int = Field(default=5, ge=1, le=20)

class SearchHit(BaseModel):
    content: str
    source: str
    distance: float | None = None

class SearchResponse(BaseModel):
    results: list[SearchHit]
    query: str
```

**Step 2: Add endpoint to `workflows/a2a_server.py`**

```python
@app.post("/search", response_model=SearchResponse)
@limiter.limit("30/minute")
async def search_documents(
    request: Request,
    body: SearchRequest,
    x_tenant_id: str = Header(default="system"),
    user: dict = Depends(validate_token),
):
    """POST /search — Semantic search across uploaded documents."""
    from workflows.embedding_client import embed_texts
    from workflows.vector_store import search

    query_embedding = (await embed_texts([body.query]))[0]
    hits = search(x_tenant_id, query_embedding, n_results=body.n)
    return {"results": hits, "query": body.query}
```

**Step 3: Write test, run, verify pass**

**Step 4: Commit**

```
git add workflows/a2a_server.py workflows/models.py
git commit -m "feat: /search endpoint — semantic search via ChromaDB"
```

---

### Task 12: Inject RAG context into chat completions

**Files:**
- Modify: `workflows/a2a_server.py` (chat_completions endpoint)

**Step 1: Add RAG context helper**

```python
async def _get_rag_context(user_message: str, tenant_id: str) -> str:
    """Query vector store for relevant document chunks."""
    try:
        from workflows.embedding_client import embed_texts
        from workflows.vector_store import search

        query_embedding = (await embed_texts([user_message]))[0]
        hits = search(tenant_id, query_embedding, n_results=3)
        if not hits:
            return ""

        parts = []
        for hit in hits:
            parts.append(f"[Source: {hit['source']}]\n{hit['content']}")
        return "\n\n---\n\n".join(parts)
    except Exception as e:
        logger.debug("RAG context fetch failed (non-fatal): %s", e)
        return ""
```

**Step 2: Wire into `/v1/chat/completions`**

In the chat_completions handler, before forwarding to budget-proxy, extract user message and inject RAG context:

```python
# Get RAG context for the latest user message
user_msgs = [m for m in chat_messages if m.get("role") == "user"]
if user_msgs:
    rag_context = await _get_rag_context(user_msgs[-1]["content"], x_tenant_id)
    if rag_context:
        rag_system = {"role": "system", "content": f"Relevant context from uploaded documents:\n{rag_context}"}
        chat_messages.insert(0, rag_system)
```

**Step 3: Write test, run, verify pass**

**Step 4: Commit**

```
git add workflows/a2a_server.py
git commit -m "feat: RAG context injection into chat completions"
```

---

## Phase 4: MCP Tools + OVMS Wiring

### Task 13: Wire real MCP tools

**Files:**
- Modify: `workflows/a2a_server.py` (/tools endpoint)

**Step 1: Replace stub tools with real tool definitions**

```python
REAL_TOOLS = [
    {
        "name": "chat",
        "description": "Send a message to the AI via LLM (GPT-4o, Claude, or local OVMS)",
        "input_schema": {
            "type": "object",
            "properties": {
                "message": {"type": "string"},
                "model": {"type": "string", "default": "gpt-4o-mini"},
            },
            "required": ["message"],
        },
    },
    {
        "name": "upload_document",
        "description": "Upload a document for RAG indexing (PDF, DOCX, TXT → parsed, chunked, embedded, stored)",
        "input_schema": {
            "type": "object",
            "properties": {
                "filename": {"type": "string"},
                "content_base64": {"type": "string"},
            },
            "required": ["filename", "content_base64"],
        },
    },
    {
        "name": "search_documents",
        "description": "Semantic search across uploaded documents via ChromaDB vector similarity",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {"type": "string"},
                "n": {"type": "integer", "default": 5},
            },
            "required": ["query"],
        },
    },
    {
        "name": "run_inference",
        "description": "Run inference on OVMS-hosted model (OpenVINO Model Server, REST port 9001)",
        "input_schema": {
            "type": "object",
            "properties": {
                "model_name": {"type": "string"},
                "input_data": {"type": "object"},
            },
            "required": ["model_name", "input_data"],
        },
    },
    {
        "name": "list_models",
        "description": "List available LLM models (budget-proxy) and OVMS models (local inference)",
        "input_schema": {"type": "object", "properties": {}},
    },
    {
        "name": "system_health",
        "description": "4-level health hierarchy: etcd, Ceph, OpenBao, OVMS, OTel",
        "input_schema": {"type": "object", "properties": {}},
    },
]
```

Update /tools handler to return these instead of trying to ping MCP servers.

**Step 2: Update test_endpoints_pydantic.py tool count assertion**

**Step 3: Commit**

```
git add workflows/a2a_server.py tests/test_endpoints_pydantic.py
git commit -m "feat: real MCP tool definitions (chat, search, inference, upload)"
```

---

### Task 14: Wire OVMS local model in budget-proxy

**Files:**
- Modify: `src/budget-proxy/proxy.py`

**Step 1: Fix LOCAL_LLM_URL default to match docker-compose**

```python
# OLD:
LOCAL_LLM_URL = os.environ.get("LOCAL_LLM_URL", "http://ovms:8000/v1")

# NEW:
LOCAL_LLM_URL = os.environ.get("LOCAL_LLM_URL", "http://ovms:9001/v1")
```

Port 9001 is OVMS REST port (matches docker-compose OVMS config: `--rest_port 9001`).

**Step 2: Ensure budget-proxy routes `local/*` models to OVMS**

Already handled by `_route_model()`:
```python
if model.startswith("local/"):
    return (LOCAL_LLM_URL, {}, model.removeprefix("local/"))
```

When a TinyLlama or other model is deployed to OVMS `models/` volume, it will be accessible via `local/tinyllama` through budget-proxy.

**Step 3: Run tests, commit**

```
git add src/budget-proxy/proxy.py
git commit -m "fix: budget-proxy LOCAL_LLM_URL port 8000→9001 (OVMS REST)"
```

---

## Phase 5: Build & Deploy

### Task 15: Run all tests and lint

**Step 1: Python tests**

Run: `cd "OneDrive/Desktop/Antigravity-Node" && python -m pytest -q`
Expected: All pass (including new test files)

**Step 2: TypeScript build**

Run: `cd "OneDrive/Desktop/Antigravity-Node/src/master-ui" && npm run build`
Expected: 0 errors

**Step 3: Ruff lint**

Run: `cd "OneDrive/Desktop/Antigravity-Node" && python -m ruff check .`
Expected: 0 errors

---

### Task 16: Build and push Docker images

**Step 1: Build all 3 images**

```bash
cd "OneDrive/Desktop/Antigravity-Node"

docker build -t us-central1-docker.pkg.dev/agentic1111/antigravity/orchestrator:v14.1 -f src/orchestrator/Dockerfile.cloud .
docker build -t us-central1-docker.pkg.dev/agentic1111/antigravity/budget-proxy:v14.1 -f src/budget-proxy/Dockerfile .
docker build -t us-central1-docker.pkg.dev/agentic1111/antigravity/ui:v14.1 -f src/master-ui/Dockerfile .
```

**Step 2: Push to GAR**

```bash
docker push us-central1-docker.pkg.dev/agentic1111/antigravity/orchestrator:v14.1
docker push us-central1-docker.pkg.dev/agentic1111/antigravity/budget-proxy:v14.1
docker push us-central1-docker.pkg.dev/agentic1111/antigravity/ui:v14.1
```

---

### Task 17: Deploy to GCP VM

**Step 1: SCP updated docker-compose.yml**

```bash
gcloud compute scp deployment/cloud-test/docker-compose.yml \
  antigravity-v14-pilot:/home/ubuntu/antigravity/repo/deployment/cloud-test/docker-compose.yml \
  --zone=us-central1-a --project=agentic1111
```

**Step 2: Pull and restart**

```bash
gcloud compute ssh antigravity-v14-pilot --zone=us-central1-a --project=agentic1111 \
  --command="cd /home/ubuntu/antigravity/repo/deployment/cloud-test && sudo docker compose pull && sudo docker compose up -d"
```

**Step 3: Verify endpoints**

```bash
# Health
curl http://34.170.105.203:1055/api/health

# Models
curl http://34.170.105.203:1055/api/v1/models

# Tools (should show 6 real tools)
curl http://34.170.105.203:1055/api/tools

# Capabilities
curl http://34.170.105.203:1055/api/capabilities
```

---

### Task 18: E2E Verification

1. Browser: `http://34.170.105.203:1055` → Chat loads INSIDE AppShell (no z-index overlap)
2. TopBar shows `v14.1` (not v13.0)
3. Chat: Send "Hello" → streaming response
4. Refresh page → messages persist (localStorage)
5. Upload a PDF → response `{"status": "uploaded", ...}`
6. Wait 5s → check server logs for "RAG indexed ... N chunks"
7. POST `/api/search` with query about PDF → returns relevant chunks
8. Chat: Ask question about uploaded PDF → answer includes document context
9. Budget: `http://34.170.105.203:1055/budget` → shows spend (embedding costs tracked)

---

## Architecture After This Plan

```
Browser :1055
    │
    ├── nginx reverse proxy
    │     ├── /api/budget/* → budget-proxy:4055
    │     │     ├── /v1/chat/completions (OpenAI/Anthropic/OVMS routing)
    │     │     ├── /v1/embeddings (text-embedding-3-small)
    │     │     ├── /v1/models (model list)
    │     │     └── /health, /history (budget tracking)
    │     │
    │     ├── /api/* → orchestrator:8080 (FastAPI)
    │     │     ├── /v1/chat/completions (+ RAG context injection)
    │     │     ├── /upload (→ Ceph S3 → parse → chunk → embed → ChromaDB)
    │     │     ├── /search (semantic search via ChromaDB)
    │     │     ├── /health (4-level hierarchy)
    │     │     ├── /tools (6 real MCP tools)
    │     │     ├── /capabilities
    │     │     ├── /task (AsyncDAGEngine submit)
    │     │     └── /handoff (A2A protocol)
    │     │
    │     └── / → React SPA (Consciousness Chamber)
    │
    ├── etcd:2379 (distributed locks, task state)
    ├── ceph-demo:8000 (S3 artifact storage)
    ├── openbao:8200 (secrets vault)
    ├── ovms:9001 (OpenVINO inference REST)
    ├── chromadb:8000 (vector search) ← NEW
    └── otel-collector (telemetry)
```
